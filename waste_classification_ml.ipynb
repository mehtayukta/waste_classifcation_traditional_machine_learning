{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/webrockerz2020/waste_classifcation_traditional_machine_learning/blob/main/waste_classification_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAcqKkHFIhsb",
        "outputId": "785b532e-e667-4c14-ff7f-32be0dbc5f77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import cv2 as cv\n",
        "import tqdm\n",
        "import threading\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "import pickle\n",
        "\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from skimage.feature import hog\n",
        "from tqdm import tqdm #add progress bars to loops and iterable objects.\n",
        "from multiprocessing import Pool\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, matthews_corrcoef\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.options.display.max_columns = None\n",
        "\n",
        "#Connecting the driver\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uzwOyDt1InGv"
      },
      "outputs": [],
      "source": [
        "# Setting all the directories\n",
        "\n",
        "root = '/content/drive/MyDrive/ML'\n",
        "model = 'models'\n",
        "#style_file = 'styles.csv'\n",
        "image_folder_train = root + '/data/train/'\n",
        "image_folder_train_o =image_folder_train + 'O/'\n",
        "image_folder_train_r =image_folder_train + 'R/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gRsNYXHqI4In"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering using HoG\n",
        "def get_all_image_names_and_its_class_to_df(folder_name):\n",
        "    #print(folder_name)\n",
        "    df_local=pd.DataFrame(columns=['id','masterCategory'])\n",
        "    for folder in os.listdir(folder_name):\n",
        "      #print(folder)\n",
        "      if folder != '.DS_Store':\n",
        "            folder_path =os.path.join(folder_name,folder)\n",
        "            #print(folder_path)\n",
        "            temp_df=pd.DataFrame(columns=['id','masterCategory'])\n",
        "            img_list=[]\n",
        "            for img in os.listdir(folder_path):\n",
        "                img_list.append(img)\n",
        "            #print(folder)\n",
        "            temp_df['id'] =img_list\n",
        "            temp_df['masterCategory']= folder\n",
        "            #print(temp_df)\n",
        "            df_local = pd.concat([df_local, temp_df], ignore_index=True)\n",
        "    return df_local\n",
        "\n",
        "# Loading the images\n",
        "def load_image(ids, path):\n",
        "    img = cv.imread(path + ids, cv.IMREAD_GRAYSCALE)\n",
        "    return img, ids\n",
        "\n",
        "def process_image(id_path_tuple):\n",
        "    id, path = id_path_tuple\n",
        "    img, id = load_image(id, path)\n",
        "    if img is not None:\n",
        "        return [img, id]\n",
        "\n",
        "# resizing of images\n",
        "def resize_image(img,ids):\n",
        "    return cv.resize(img, (80, 80),interpolation =cv.INTER_LINEAR) # Tired with 60,80 # tired 60,60 # tired 100 by 100 system crash\n",
        "\n",
        "#bluring the images\n",
        "def hog_to_blur_the_images(image):\n",
        "    ppcr = 7 # tired with 8,7,5\n",
        "    ppcc = 7 # Tired with 8,7,5\n",
        "    blur = cv.GaussianBlur(image, (5, 5), 0) #kernal size 7,8 checked , # larger kernel more blur # changes sigma from 0\n",
        "    fd, hog_image = hog(blur, orientations=5, pixels_per_cell=(ppcr, ppcc), cells_per_block=(2, 2), block_norm='L2', visualize=True)\n",
        "    return hog_image, fd\n",
        "\n",
        "#get the edges\n",
        "def get_edges(img_list,n_samples):\n",
        "  edges = [cv.Canny(image,50,150,apertureSize = 3) for image in img_list]\n",
        "  #edges = [cv.Canny(image, 50, 200, apertureSize=3) for image in img_list]\n",
        "  #edges = [cv.Canny(image, 50, 250, apertureSize=3) for image in img_list]\n",
        "  edges = np.array(edges)\n",
        "  #print(edges)\n",
        "  n_samples_edges = len(edges)\n",
        "  edge_images_train = edges.reshape((n_samples, -1))\n",
        "  edge_images_train.shape\n",
        "  return edge_images_train\n",
        "\n",
        "#pixel intensity\n",
        "def get_pixel_intnsity(img_list):\n",
        "  histr_train = [cv.calcHist([img],[0],None,[256],[0,256]) for img in img_list] # bin size,color channel\n",
        "  #histr_train = [cv.calcHist([img],[0],None,[300],[0,300]) for img in img_list]\n",
        "  histr_train = np.array(histr_train)\n",
        "  n_samples_histr_train = len(histr_train)\n",
        "  image_hist_train = histr_train.reshape((n_samples_histr_train, -1))\n",
        "  image_hist_train.shape\n",
        "  return image_hist_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "j2NXEOJrXUYa"
      },
      "outputs": [],
      "source": [
        "#Multiproessing\n",
        "# Mulitprocessing 1 --> Process Image\n",
        "def create_a_multiprocessing(folder_name,df):\n",
        "    df_ids = list(df.id)\n",
        "    # Create a list of (id, path) tuples\n",
        "    id_path_tuples = [(id, folder_name) for id in df_ids]\n",
        "    # Create a multiprocessing pool\n",
        "    pool = Pool(processes=8)\n",
        "    # Process images in parallel\n",
        "    results = list(tqdm(pool.imap(process_image, id_path_tuples), total=len(df_ids)))\n",
        "    # Close the pool\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    images= [result for result in results if result is not None]\n",
        "    len(images)\n",
        "    return images\n",
        "\n",
        "# Multiprocessing 2 --> Blurring Image\n",
        "def create_a_multiprocessing_blur(image_lst):\n",
        "    pool = Pool(processes=5)\n",
        "    hog_images, hog_features = zip(*pool.map(hog_to_blur_the_images, image_lst))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "\n",
        "    hog_features = np.array(hog_features)\n",
        "    return hog_images, hog_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WV0DJApJBgA",
        "outputId": "b6798a06-e266-4d70-e9d8-21f8f601dbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25077, 2)\n",
            "           id masterCategory\n",
            "0  R_9506.jpg              R\n",
            "1  R_9613.jpg              R\n",
            "2  R_9751.jpg              R\n",
            "3  R_9872.jpg              R\n",
            "4  R_9354.jpg              R\n",
            "5  R_9943.jpg              R\n",
            "6  R_9265.jpg              R\n",
            "7  R_9280.jpg              R\n",
            "8  R_9287.jpg              R\n",
            "9  R_9210.jpg              R\n",
            "O    13966\n",
            "R    11111\n",
            "Name: masterCategory, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Loading all the images\n",
        "df_train = get_all_image_names_and_its_class_to_df(image_folder_train)\n",
        "print(df_train.shape)\n",
        "print(df_train.head(10))\n",
        "print(df_train['masterCategory'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfcoeU9CLdbd",
        "outputId": "67b8d470-b589-4211-ce4c-8df2fea8f5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25077/25077 [01:28<00:00, 282.96it/s]\n",
            "100%|██████████| 25077/25077 [00:43<00:00, 581.71it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train images 25077\n"
          ]
        }
      ],
      "source": [
        "train_images_o =create_a_multiprocessing(image_folder_train_o,df_train)\n",
        "train_images_r =create_a_multiprocessing(image_folder_train_r,df_train)\n",
        "all_train_images = train_images_o+train_images_r\n",
        "print(\"Length of train images\",len(all_train_images))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y99-aj2CL03w"
      },
      "outputs": [],
      "source": [
        "all_images_resized_train = [[resize_image(x,y),y] for x,y in all_train_images]\n",
        "df_labels_train = pd.DataFrame(all_images_resized_train,columns=['image','id'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BkySq0aP5wx",
        "outputId": "a9749772-e035-4bae-829c-777131702db0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data Shape:  (25077, 4)\n",
            "O    13966\n",
            "R    11111\n",
            "Name: masterCategory, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Adding all the labels\n",
        "target = 'masterCategory'\n",
        "df_labels_train = pd.merge(df_labels_train,df_train,how='left',on=['id'])\n",
        "df_labels_train = df_labels_train.fillna('Others')\n",
        "df_labels_train['class'] = pd.factorize(df_labels_train[target])[0]\n",
        "print(\"Data Shape: \", str(df_labels_train.shape))\n",
        "print(df_labels_train[target].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmlsOtEUQDi5",
        "outputId": "ddc15a08-82a2-4fad-b647-68c85b210544"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25077\n"
          ]
        }
      ],
      "source": [
        "train_images = np.stack(df_labels_train.image.values,axis=0)\n",
        "n_samples_train = len(train_images)\n",
        "print(n_samples_train)\n",
        "data_images_train = train_images.reshape((n_samples_train, -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd43Pgv9QT-u"
      },
      "outputs": [],
      "source": [
        "hog_images_train, hog_features_train = create_a_multiprocessing_blur(train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qq-RFRM0QWZC"
      },
      "outputs": [],
      "source": [
        "for img in hog_images_train[:2]:\n",
        "    plt.imshow(img)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZut55GCRUyj"
      },
      "outputs": [],
      "source": [
        "#getting the edges\n",
        "edge_images_train = get_edges(train_images,n_samples_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HZmvF36RZuD"
      },
      "outputs": [],
      "source": [
        "train_images.shape, hog_features_train.shape, edge_images_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJ2rNLssReQg"
      },
      "outputs": [],
      "source": [
        "edge_hog_train = np.hstack([hog_features_train,edge_images_train]) #to stack the sequence of input arrays horizontally (i.e. column wise) to make a single array\n",
        "edge_hog_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkAmu3HSRg7t"
      },
      "outputs": [],
      "source": [
        "#getting pixel intensity\n",
        "image_hist_train = get_pixel_intnsity(train_images)\n",
        "image_hist_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7aeJmRV5x88"
      },
      "outputs": [],
      "source": [
        "edge_hog = np.hstack([hog_features_train,edge_images_train,image_hist_train])\n",
        "edge_hog.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_images #--> freeing up the space"
      ],
      "metadata": {
        "id": "zgB24AslbSF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn import datasets, svm, metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.decomposition import PCA\n"
      ],
      "metadata": {
        "id": "SYYjA7mPWge4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ7Rxn2KRjlJ"
      },
      "outputs": [],
      "source": [
        "# Models\n",
        "X_train, X_test, y_train, y_test = train_test_split(hog_features_train, df_train['masterCategory'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**"
      ],
      "metadata": {
        "id": "LsxnipIGWz0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_measures(y_pred,y_test,pos):\n",
        "  # Sensitivity (Recall)\n",
        "    sensitivity = recall_score(y_test, y_pred, pos_label=pos)  # or pos_label='O' depending on your positive class\n",
        "    print(\"Sensitivity (Recall):\", sensitivity)\n",
        "\n",
        "    # Precision\n",
        "    precision = precision_score(y_test, y_pred, pos_label=pos)  # or pos_label='O' depending on your positive class\n",
        "    print(\"Precision:\", precision)\n",
        "\n",
        "    # F1-score\n",
        "    f1 = f1_score(y_test, y_pred, pos_label=pos)  # or pos_label='O' depending on your positive class\n",
        "    print(\"F1-score:\", f1)\n",
        "\n",
        "    # Cohen's Kappa statistic\n",
        "    kappa = cohen_kappa_score(y_test, y_pred)\n",
        "    print(\"Cohen's Kappa:\", kappa)\n",
        "\n",
        "    # Matthews Correlation Coefficient (MCC)\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
        "\n",
        "    # Confusion Matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "HoBBxbnoXqVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOoAbp4URyzp"
      },
      "outputs": [],
      "source": [
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)\n",
        "y_train.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypBmd3CRSHPF"
      },
      "outputs": [],
      "source": [
        "# Testing with different values of K\n",
        "test_accuracy = []\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "lst_of_k= [3,5,7,10,15,20,25]\n",
        "for i in lst_of_k:\n",
        "  print(i)\n",
        "  classifier = KNeighborsClassifier(n_neighbors=i,algorithm='brute')\n",
        "  classifier.fit(X_scaled, y_train)\n",
        "  y_pred = classifier.predict(scaler.transform(X_test))\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the Classifer for best Value of K =20\n",
        "classifier = KNeighborsClassifier(n_neighbors=20, algorithm='brute')\n",
        "classifier.fit(X_scaled, y_train)\n",
        "test_accuracy = classifier.score(scaler.transform(X_test), y_test)\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred,'R')\n",
        "accuracy_measures = (y_test, y_pred,'O')"
      ],
      "metadata": {
        "id": "U7w-O2cjrMeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying PCA as the results were not good\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Apply PCA\n",
        "n_components = 2000\n",
        "pca = PCA(n_components=n_components)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# Initialize and train the KNN classifier\n",
        "k_neighbors = 20\n",
        "classifier = KNeighborsClassifier(n_neighbors=k_neighbors, algorithm='brute')\n",
        "classifier.fit(X_train_pca, y_train)\n",
        "y_pred = classifier.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred,'R')\n",
        "accuracy_measures = (y_test, y_pred,'O')"
      ],
      "metadata": {
        "id": "ipiz3ma1Yhet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saved the model using Pickel\n",
        "\n",
        "with open(os.path.join(root,model,'knn_model_brute.pkl'), 'wb') as knnPickle:\n",
        "    pickle.dump(classifier, knnPickle)\n",
        "\n",
        "# Load the model from disk\n",
        "loaded_model = pickle.load(open(os.path.join(root,model,'knn_model_brute.pkl'), 'rb'))\n"
      ],
      "metadata": {
        "id": "91ITW9oF2-vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwQeLHzzxjLa"
      },
      "outputs": [],
      "source": [
        "\"\"\"classifier = KNeighborsClassifier(n_neighbors=20,algorithm='kd_tree')\n",
        "classifier.fit(X_scaled, y_train)\n",
        "test_accuracy = classifier.score(scaler.transform(X_test), y_test)\n",
        "print(test_accuracy)\"\"\"\n",
        "\n",
        "\"\"\"classifier = KNeighborsClassifier(n_neighbors=20,algorithm='ball_tree')\n",
        "classifier.fit(X_scaled, y_train)\n",
        "test_accuracy = classifier.score(scaler.transform(X_test), y_test)\n",
        "print(test_accuracy)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the new image path\n",
        "\n",
        "test_image = 'test_image/'\n",
        "image = \"pla.jfif\"\n",
        "image_name = image.split('.')[0]\n",
        "image_name = image_name + \".jpeg\"\n",
        "new_image_path = root_dir + \"/\" + test_image + image\n",
        "# Converting the Image to JPEG\n",
        "\n",
        "im = Image.open(os.path.join(root_dir, test_image, image))\n",
        "rgb_im = im.convert('RGB')\n",
        "rgb_im.save(os.path.join(root_dir, test_image, image_name))\n",
        "print(\"Image saved successfully ...\")\n",
        "\n",
        "# Load and preprocess the new image\n",
        "X_new = Image.open(os.path.join(root_dir,test_image,image_name))\n",
        "plt.imshow(X_new)\n",
        "new_image = cv.imread(new_image_path, cv.IMREAD_GRAYSCALE)\n",
        "new_image = cv.resize(new_image, (60, 80), interpolation=cv.INTER_LINEAR)  # Resize if needed\n",
        "# Extract HOG features\n",
        "ppcr = 7\n",
        "ppcc = 7\n",
        "blur = cv.GaussianBlur(new_image, (7, 7), 1)\n",
        "fd, hog_features = hog(blur, orientations=5, pixels_per_cell=(ppcr, ppcc), cells_per_block=(2, 2), block_norm='L2', visualize=True)\n",
        "stacked_features = np.stack([hog_features], axis=0)\n",
        "reshaped_features = stacked_features.reshape((1, -1))\n",
        "if reshaped_features.shape[1] != X_train.shape[1]:\n",
        "    reshaped_features = reshaped_features[:, :X_train.shape[1]]\n",
        "\n",
        "new_image_scaled = scaler.transform(reshaped_features)\n",
        "new_image_pca = pca.transform(new_image_scaled)\n",
        "predicted_class = classifier.predict(new_image_pca)\n",
        "print(\"Predicted Class:\", predicted_class)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ozi7xooI9RBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Done with KNN"
      ],
      "metadata": {
        "id": "2Oxkb8RzFU6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAstwGcKTIJn"
      },
      "outputs": [],
      "source": [
        "#Logistic regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "test_accuracy = []\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "classifier = LogisticRegression(max_iter=5)\n",
        "classifier.fit(X_scaled, y_train)\n",
        "test_accuracy = classifier.score(scaler.transform(X_test), y_test)\n",
        "print(test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDbW53CRTl4u"
      },
      "outputs": [],
      "source": [
        "#Random Forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLE8UM32gKhG"
      },
      "outputs": [],
      "source": [
        "rfClassifier = RandomForestClassifier()\n",
        "rfClassifier.fit(X_train, y_train)\n",
        "rfClassifier_accuracy = rfClassifier.score(X_test, y_test)\n",
        "print('Accuracy of the Random Forest Classifier is: ', rfClassifier_accuracy)\n",
        "print('\\n')\n",
        "\n",
        "cv_scores = cross_val_score(rfClassifier, X_test, y_test, cv=5)\n",
        "print('Scores from cross-validation is: ', cv_scores)\n",
        "print('Average accuracy from cross-validation is: {}'.format(np.mean(cv_scores)))\n",
        "print('\\n')\n",
        "\n",
        "predictions = rfClassifier.predict(X_test)\n",
        "print('---------- Model evaluation ----------')\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTZ1vAb_ozzc"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled_train = scaler.fit_transform(X_train)\n",
        "X_scaled_test = scaler.transform(X_test)\n",
        "\n",
        "rfClassifier = RandomForestClassifier()\n",
        "rfClassifier.fit(X_scaled_train, y_train)\n",
        "rfClassifier_accuracy = rfClassifier.score(X_scaled_test, y_test)\n",
        "print('Accuracy of the Random Forest Classifier is: ', rfClassifier_accuracy)\n",
        "print('\\n')\n",
        "\n",
        "cv_scores = cross_val_score(rfClassifier, X_scaled_test, y_test, cv=5)\n",
        "print('Scores from cross-validation is: ', cv_scores)\n",
        "print('Average accuracy from cross-validation is: {}'.format(np.mean(cv_scores)))\n",
        "print('\\n')\n",
        "\n",
        "predictions = rfClassifier.predict(X_scaled_test)\n",
        "print('---------- Model evaluation ----------')\n",
        "print(classification_report(y_test, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwsT7fjQCGIe"
      },
      "outputs": [],
      "source": [
        "### Tuning hyperparams for the Random Forest Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "707827SfDMQS"
      },
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "\t'n_estimators': [25, 50, 100, 150, 200],\n",
        "\t'max_features': ['sqrt', 'log2', None],\n",
        "\t'max_depth': [3, 6, 9, None],\n",
        "\t'max_leaf_nodes': [3, 6, 9, None],\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWCSHQ2rCSMx"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIesx9o7CGT7"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "random_search = RandomizedSearchCV(RandomForestClassifier(), param_grid)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(random_search.best_estimator_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRuKHIWGCJtN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzW7GblUAUfV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTlN8W2MLnz6HXkALEqHOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}